# -*- coding: utf-8 -*-
"""CNN for Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iJ735IjsRrbP-zEYLh17AsTctwl4spa

# Step 1: Installation and Setup
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Step 2:Importing the dataset from kaggle to google colab"""

# install kaggle API
! pip install kaggle

# Creating a directory as kaggle

! mkdir -p ~/.kaggle

# import kaggle API
from google.colab import files
uploaded = files.upload()

# copying the API key to kaggle directory
! cp kaggle.json ~/.kaggle

# disable the API key
! chmod 600 /root/.kaggle/kaggle.json

# list of datasets
! kaggle datasets list

# import the dataset
! kaggle datasets download -d mlg-ulb/creditcardfraud

# unzipping dataset
! unzip /content/creditcardfraud.zip

dataset_1 = pd.read_csv('/content/creditcard.csv')

dataset_1.head()

len(dataset_1.columns)

"""# Step 3: Data Preprocessing"""

dataset_1.shape

# Checking null values
dataset_1.isnull().sum()

dataset_1.info()

# Observation in each class
dataset_1['Class'].value_counts()

"""The given dataset is highly unbalanced"""

# Balanced the dataset

# Given 1 for fraud transaction
# and 0 for non fraud transaction

fraud = dataset_1[dataset_1['Class']==1]
non_fraud = dataset_1[dataset_1['Class']==0]

fraud.shape

non_fraud.shape

# To balance this dataset there should be almost eqaul transaction in each class

# random selection of samples
non_fraud_t = non_fraud.sample(n=492)

non_fraud_t.shape

# merge datasets
dataset = pd.concat([fraud, non_fraud_t], ignore_index=True)

print(dataset)

# Observation in each class
dataset['Class'].value_counts()

"""Equal number of observation in each class"""

# matrix of features / independent variable

x = dataset.drop(labels=['Class'], axis = 1)

# Dependent variable

y= dataset['Class']

x.shape, y.shape

# splitting the dataset into train and test set

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=0)

x_train.shape, x_test.shape

# feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

# Converting y_train and y_test to numpy array
# because they have only one column

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

x_train.shape, x_test.shape

"""Here x_test and x_train are only in 1D and we have to convert them in 2D

We have to change the dimension because the CNN layer will take the dataset only in this way
"""

# reshape the dataset

x_train = x_train.reshape(787, 30, 1)
x_test = x_test.reshape(197, 30, 1)

x_train.shape, x_test.shape

"""# Step 4: Building the model"""

# define an object

model = tf.keras.models.Sequential()

# first CNN layer

model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=2, padding='same', activation = 'relu', input_shape = (30,1)))

# batch normalization
# used to normalise the inputs of each layer
# due to batch normalization speed , stability and performace of the network increses

model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
# selects the maximum layer from each batch

model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
# regularization technique
# we are ignoring 20 percent neurons while training
# these 20 percent is randomly selected

model.add(tf.keras.layers.Dropout(0.2))

# second CNN layer

model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=2, padding='same', activation = 'relu'))

# batch normalization
# used to normalise the inputs of each layer
# due to batch normalization speed , stability and performace of the network increses

model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
# selects the maximum layer from each batch

model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
# regularization technique
# we are ignoring 20 percent neurons while training
# these 20 percent is randomly selected

model.add(tf.keras.layers.Dropout(0.3))

# flatten layer
# converts array into a vector

model.add(tf.keras.layers.Flatten())

# first dense layer
model.add(tf.keras.layers.Dense(units = 64, activation ='relu'))

# dropout layer
model.add(tf.keras.layers.Dropout(0.3))

# output layer
model.add(tf.keras.layers.Dense(units =1, activation = 'sigmoid'))

model.summary()

# Compiling the model

opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)

model.compile(optimizer = opt, loss ='binary_crossentropy', metrics=['accuracy'])

"""# Step 5: Training the model"""

history = model.fit(x_train, y_train, epochs = 25, validation_data = (x_test, y_test))

# model predictions
# Get predicted probabilities
y_pred_prob = model.predict(x_test)

# Apply threshold to convert probabilities to class labels (0 or 1)
y_pred = (y_pred_prob > 0.5).astype("int32")

print(y_pred[12]), print(y_test[12])

# confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

# calculating accuracy using confusion matrix

acc_cm = accuracy_score(y_test, y_pred)
print(acc_cm)

"""# Step 6: Learning Curve"""

def learning_curve(history, epoch):

  # training vs validation accuracy
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

  # training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

learning_curve(history, 25)

